# 0.Paper

这里写google上的学术高引用论文。

# 01.Teacher–Student Curriculum Learning

[Teacher–student curriculum learning](https://ieeexplore.ieee.org/abstract/document/8827566/)

这篇论文提出的教师学生课程学习是一个自动课程学习框架，属于TSCL。学生会学习一项复杂地任务，而老师则是将困难的任务分成简单的子任务让学生进行训练。在这里教师需要给学生安排学习进步最快的任务即学习曲线斜率最高的地方，此外还需要解决学生表现变差的遗忘的问题。

>  作者指出来进行课程学习需要面对的三个问题：1.需要能够对子任务难度排序2.需要制定难易的标准门槛3.需要吧不同难度的任务混合以免遗忘

论文的核心思想是***让学生模型学习进度最快的任务就是曲线斜率最高，为防止遗忘，学生还应该练习表现越差的任务就是曲线斜率为负的时候***。（这里是否可以利用**LSTM**呢，如何结合呢）

然后是实现的四种算法：1.***meanteacher的思想，指数加权平均的算法***2.多次练习得到累计的分数，用线性回归估计学习曲线长度3.窗口算法，保存缓冲区，以时间步长作为输入用线性回归估计任务地学习曲线斜率，然后选取4.抽样算法，选择缓冲区里采样最近的最高的抽样奖励。

> https://www.youtube.com/watch?v=GFCujBpTf3k

# 02.Automated curriculum learning for neural networks

> [Automated curriculum learning for neural networks](https://proceedings.mlr.press/v70/graves17a.html)

这篇论文更偏重理论性，有一些看不懂，就先看一下作者得出的结论吧。

论文主要提出的是一个方法：ACL自动课程学习，其***核心思想是一种通过动态调整训练实例难度来增强神经网络训练的技术。使课程适应网络的学习速度和提升泛化能力，优化网络提升性能***。

传统的方法是使用固定的课程按照提前设定好的困难程度排序输入模型，但这样并不是最佳的，他会导致学习效率低下和泛化能力不佳，此时需要动态调整模型。

ACL的算法随着网络的学习和改进不断增加实例的难度，让网络从易到难学习，避免网络陷入局部最优和无法泛化的情况。***在实现ACL的方法上，可以使用网络性能来确定示例的难度，或者使用强化学习来优化课程本身***。

# 03.Self-Paced Curriculum Learning

> [Self-paced curriculum learning](https://ojs.aaai.org/index.php/AAAI/article/view/9608)

同上这也是一篇理论指导类的，也是收录在AAAI，然后也是综述里面提到的SPCL，自步课程学习，上一篇提出的是ACL的理论，这里是SPCL的理论基础。
SPCL是融合了SPL和CL而产生的。

这个摘要写得太好了，把英文原文和中文原文都复制过来。这个摘要让我明白了这三个的概念和着重点以及作者提出的SPCL的意义。

- Curriculum learning (CL) or self-paced learning (SPL) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex sam- ples in training. The two methods share a similar con- ceptual learning paradigm, but differ in specific learn- ing schemes. In CL, the curriculum is predetermined by prior knowledge, and remain fixed thereafter. Therefore, this type of method heavily relies on the quality of prior knowledge while ignoring feedback about the learner. In SPL, the curriculum is dynamically determined to ad- just to the learning pace of the leaner. However, SPL is unable to deal with prior knowledge, rendering it prone to overfitting. In this paper, we discover the missing link between CL and SPL, and propose a unified framework named self-paced curriculum leaning (SPCL). SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training. In comparison to human education, SPCL is analogous to “instructor-student-collaborative” learning mode, as op- posed to “instructor-driven” in CL or “student-driven” in SPL. Empirically, we show that the advantage of SPCL on two tasks.
- 课程学习（CL）或自定进度学习（SPL）代表了最近提出的一种学习制度，其灵感来自于人类和动物的学习过程，在训练中逐渐从简单的样本过渡到更复杂的样本。 这两种方法具有相似的概念学习范式，但在具体的学习方案上有所不同。 在 CL 中，课程是由先验知识预先确定的，并且此后保持固定。 因此，这种方法严重依赖先验知识的质量，而忽略了学习者的反馈。 在 SPL 中，课程是动态确定的，以适应学习者的学习进度。 然而，SPL 无法处理先验知识，因此容易出现过度拟合。 在本文中，我们发现了 CL 和 SPL 之间缺失的环节，并提出了一个统一的框架，称为自定进度课程学习（SPCL）。 ***SPCL 被表述为一个简洁的优化问题，它考虑了训练前已知的先验知识和训练期间的学习进度***。 与人类教育相比，***SPCL类似于“师生协作”的学习模式，而不是CL中的“教师驱动”或SPL中的“学生驱动”***。 根据经验，我们展示了 SPCL 在两项任务上的优势。

这篇论文提出的框架解决了CL受限于先验知识，SPL受限于自步函数，结合两者形成的SPCL即可用先验知识还可以动态调整自步函数，泛化模型。

# 04.On The Power of Curriculum Learning in Training Deep Networks

自己写一下这篇文章的摘要，使用课程学习对样本采用非均匀采样，从一些教师模型中进行迁移学习然后引导模型提升学习速度和提高性能，然后研究了不同的步调函数。

~~今天看完这一篇然后学一下看一下迁移学习和强化学习的相关博客了解一下就OK了。这个写到另外的文件夹笔记里面分开。~~

这篇文章讲了在训练深度神经网络中使用课程学习的的作用。通过逐渐增加样本难度以及评估难度，还有对比了三个pacing函数，得出就是使用课程学习有用芜湖！这篇理论的文章怎么说难怪没有中文博客。

# 05.Curriculum Learning of Multiple Tasks

14年的文章有些太老了，这篇也是主要讲理论性的，看一下摘要，综述和方法就行。

按顺序学习，你想就是课程学习的作用是什么，不就是整合总结一下之前学过的知识，把权重迁移过来。按顺序学习提出的新概念和任务，多任务之前需要把任务的难度分级然后从以到难学习。

> 作者在论文中提出了一种顺序学习多任务的方法，并研究了学习顺序对整体性能的影响。提出了一个泛化界限的理论结果，用于评估学习顺序的质量。

> 1. 顺序学习多个任务可能比同时学习这些任务更有效
> 2. 解决任务的顺序影响了整体分类性能
> 3. 他们的方法能够自动发现有益的学习顺序。

论文中也指出了模型的一个局限性，即当前模型只允许从前一个任务转移到解决当前任务，因此它输出了一系列相关任务或多个任务子序列。

# 06.Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey

是20年的引用量达480的强化学习应用在课程学习之后的框架和综述文章，可以叫做RCL吧。作者也联系了迁移学习transfer Learning。

在强化学习中有四种例子让学习更有效率：多任务学习，终身/持续学习，主动学习，元学习。其实终身/持续学习可以当作多任务的在线学习版本。主动学习主要是应用在半监督中的，模型通过评估函数计算困难程度选择样本主动学习，也可以当作是终身学习的一部分。而元学习的目标是训练代理的各种能力，使其能够在少量梯度下降内快速适应新任务。快速适应，这不就是速成嘛，给定时间来快速学习然后识别。而终身学习的话学习的顺序其实没有那么重要毕竟全部样本都需要学习的，而元学习让其在少量信息下快速学习。

总结的话这篇文章就是在强化学习的情况下，课程学习是由三部分构成的方法：任务生成，排序，迁移学习。在其中的排序使用的思想的话就有多任务学习，元学习这两种，终身学习和主动学习归类到多任务学习的方法之中。

# 07.Source Task Creation for Curriculum Learning

>  本文探讨了如何为课程学习自动创建源任务。在强化学习中，课程学习通过依次训练一系列子任务来加速学习复杂任务。
>
> **任务生成**: 通过分析任务之间的关系（例如，任务的相似性和难度）来自动生成源任务。
>
> **任务排序**: 基于任务之间的关系和难度，构建任务图，并通过拓扑排序确定任务的训练顺序。
>
> **任务优化**: 使用强化学习算法在生成的任务上进行训练，以优化代理的表现
>
> 课程学习的主要步骤基本就是上面这三个，具体的实验和框架相关。

# 08.Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks

迁移学习在课程学习上的应用，讲述了理论和实验使用深度网络。谷歌上的高引用量文章基本都是理论性的文章，这些文章理论性很强，对于初学者来说的话其实有些吃力。最多的收获应该是知道了迁移学习在课程学习上是有作用的，具体怎么使用怎么搭这个积木虽然这篇文章介绍了理论和实验但是不结合实际例子其实还是难以理解。但是搭建好的其实又发不了好的期刊写出来之后的引用量也不会很大。

从上一篇还是上一篇其实就应该换一下论文的学习思路了，这些理论性的文章需不需要？需要，但是不是现在，现在不需要很精细地去阅读理论的文章你也看不懂，最合适的就是宽泛地了解，稍微总结一下就好，比如在这一篇和之前的学习中你知道课程学习和强化学习分不开，强化学习的实现方法和迁移学习，主动学习，元学习，持续学习等等这些有联系，还有半监督中的meanteacher与课程学习也有关系以及ACL的核心思想，也算是一种方法以及自步函数，还有融合了CL和SPL的SPCL方法这些你只需要知道：哦，课程学习需要和它们结合，和它们结合之后的效果非常好，解决了怎样的问题，有哪些应用，至于是如何结合的实验和理论结果是如何写出来的，你要强行理解很难。现在就是可以多看一下别人提出一个怎样的花里胡哨的模型结合了上面提到的理论方法，哪怕是一种两种相互结合的模型，这些虽然不能给你带来一些创新性的想法，但是可以让你知道这些方法实际上是如何融合的，是如何把这些公式理论放到一起的。

>  本文结合转移学习和课程学习，提出了一种新的课程学习框架。通过理论分析和深度网络实验，作者展示了此框架如何利用预训练模型中的知识，逐步引入难度更高的任务，最终提高模型在目标任务上的表现。实验结果表明，该方法在多种任务上均取得了显著的性能提升。
>
> **方法实现**:
>
> - **预训练模型**: 先在源任务上训练一个预训练模型，以获取初始的知识。
> - **任务排序**: 根据任务的难度或相似性，将目标任务分解为一系列子任务，并按照从易到难的顺序进行训练。
> - **转移学习**: 在每个子任务上进行微调，利用之前子任务的知识来加速当前任务的学习过程。
> - **渐进训练**: 随着任务难度的增加，逐步引入更复杂的任务，直到完成目标任务的训练。

# 09.CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition

同第八篇讲的思路，理论性文章那种不提出模型的目前只需要知道他和课程学习的关系以及他运用了哪些理论方法达到了比较好的效果。

> 翻译自论文摘要：
>
> 作为人脸识别的一个新兴课题，设计基于边缘的损失函数可以增加不同类别之间的特征边缘，从而增强可辨别性。最近，基于挖掘的策略的思想被用来强调被错误分类的样本，并取得了令人欣喜的效果。然而，在整个训练过程中，先前的方法要么没有根据样本的重要性明确地强调样本，导致难样本没有得到充分利用；要么即使在训练的早期阶段就明确强调半难/难样本的影响，这可能导致收敛问题。在本文中，我们提出了一种新颖的自适应课程学习损失（CurricularFace），将课程学习的思想嵌入到损失函数中，以实现一种新颖的深度人脸识别训练策略，该策略主要解决早期训练阶段的简单样本和后期的难样本。具体来说，我们的CurricularFace会在不同的训练阶段自适应地调整易样本和难样本的相对重要性。在每个阶段，根据不同样本的难度为其分配不同的重要性。在流行基准上进行的大量实验结果证明了我们的 CurricularFace 优于最先进的竞争对手。

也就是说这篇论文针对人脸识别提出了一个损失函数，这个损失函数其实就是自步函数，ACL自动调整区分样本难度。也就是说单一提出一个损失函数就可以发cvpr了？这个是基于边缘检测提出来的损失函数，这个放这里，其他没有什么好看的，主要是如何提出的这个损失函数有学习的意义。但如果我的领域也是做人脸识别的话就需要仔细看一下，毕竟相同领域的论文是需要做实验对比的。



# 10.Curriculum Learning for Natural Language Understanding

看标题就知道是NLP中的课程学习，像这种简洁的标题只能是大佬写出来，就像综述一样只能大佬写。

明天看一下b站的如何学习论文，这个文章不知道该往哪方面学啊，我觉得不光是论文讲了啥吧，还有论文的思路。

> 本文提出了一种课程学习方法，旨在提升自然语言理解任务中的模型性能。基于预训练语言模型，该方法在微调阶段按照从易到难的顺序引入训练样本。实验表明，这种课程学习方法在多个自然语言理解任务上获得了显著的性能提升。

> - **预训练模型**: 使用预训练的语言模型（例如BERT）作为基础。
> - **样本排序**: 按照样本的难度将训练数据分为多个阶段，从易到难进行排序。
> - **分阶段训练**: 在初期阶段使用简单样本进行微调，然后逐步引入难度更高的样本，以提高模型的泛化能力。
> - **动态调整**: 在训练过程中，根据模型的表现动态调整样本的顺序和难度。



# 11.Automatic Curriculum Learning For Deep RL: A Short Survey

好了又是一篇小综述，这个是ACL在RL中的简短综述，其实我觉得这篇文章的意义更在于指明方向，诶就是你看我可以在RL上使用ACL达到更好的效果，而且并不复杂，现在的主流工作都是这样的，重要的是ACL的思想我觉得。

> - **任务生成与排序**: 自动生成和排序任务，根据代理的当前能力动态调整任务难度。
> - **自适应调整**: 在训练过程中，根据代理的表现自适应地调整任务的顺序和难度。
> - **优化策略**: 使用强化学习算法在生成的任务上进行训练，以优化代理的表现。
> - **多样性与探索**: 确保任务的多样性和探索性，以促进代理的泛化和学习效率。

# 12.Dynamic Curriculum Learning for Imbalanced Data Classification

针对类不均衡提出来的动态课程学习，这篇文章是商汤的，值得看一下，而且类不均衡在分割上本来就是一个大问题。还有我觉得这个论文的笔记是需要分类了，主要是mac的没有大纲。

> - **样本权重调整**: 动态调整样本的权重，使模型在训练过程中逐步从易到难进行学习。
> - **阶段性训练**: 将训练过程分为多个阶段，在每个阶段中主要处理特定难度的样本。
> - **不平衡数据处理**: 专门针对不平衡数据设置不同的损失权重，以提高类别不平衡情况下的分类性能。
> - **动态调整策略**: 根据模型的训练进展，动态调整样本的权重和顺序，以确保模型能够有效地学习到不平衡数据的特征。



# 13.Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning

内在动机目标探索过程（IMGEP）结合了自动课程学习的理念。采用内在动机机制，自主生成学习目标。明天把论文的笔记分类然后找一下比如一二区的那种组合论文，这种理论论文太深奥了，一二区论文看的是模型如何构建，代码的话有清楚的就看一下，看别人是如何把方法结合的。
